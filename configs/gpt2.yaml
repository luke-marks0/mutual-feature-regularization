experiment: gpt2

hyperparameters:
  input_size: 768
  hidden_size: 3072
  k_sparse: 32
  num_saes: 1
  learning_rate: 0.001
  num_epochs: 1
  weight_decay: 0.01
  auxiliary_loss_weight: 0
  ensemble_consistency_weight: 0
  data_collection_batch_size: 200
  training_batch_size: 200
  num_samples: 300000
