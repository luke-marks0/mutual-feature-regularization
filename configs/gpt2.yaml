experiment: gpt2

hyperparameters:
  input_size: 768
  hidden_size: 3072
  k_sparse: 32
  num_saes: 2
  learning_rate: 0.001
  num_epochs: 1
  weight_decay: 0.01
  auxiliary_loss_weight: 0.1
  ensemble_consistency_weight: 0.1
  data_collection_batch_size: 15
  training_batch_size: 500
  num_samples: 1500
